# Word2Vec

https://arxiv.org/abs/1301.3781

Word embedding is where words or phrases from the vocabulary are mapped to vectors of real numbers.

Word2Vec is a technique to find continuous embeddings for words. It learns from reading massive amounts of text and memorizing which words tend to appear in similar contexts. After being trained on enough data, it generates a 300-dimension vector for each word in a vocabulary, with words of similar meaning being closer to each other.

