{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization under Linguistic Features\n",
    "\n",
    "Link [here](https://spacy.io/usage/linguistic-features#tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading language model\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding special case Tokenization rules\n",
    "# https://spacy.io/usage/linguistic-features#special-cases\n",
    "\n",
    "# Importing necessary symbols from spacy\n",
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "\n",
    "# New sentence wanted to tokenize\n",
    "doc = nlp(u'gimme that')\n",
    "assert [w.text for w in doc] == [u'gimme', u'that'] # current tokenization has only 2 tokens\n",
    "\n",
    "# add special case rule\n",
    "special_case = [{ORTH: u'gim', LEMMA: u'give', POS: u'VERB'}, {ORTH: u'me'}]\n",
    "\n",
    "# Adding the special case to tokenizer, which will be affective from next sentence parsing\n",
    "nlp.tokenizer.add_special_case(u'gimme', special_case)\n",
    "\n",
    "assert [w.text for w in nlp(u'gimme that')] == [u'gim', u'me', u'that'] # After customization got 3 tokens\n",
    "# Pronoun lemma is returned as -PRON-!\n",
    "assert [w.lemma_ for w in nlp(u'gimme that')] == [u'give', u'-PRON-', u'that']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The special case doesn't have to match an entire whitespace-delimited substring. \n",
    "# The tokenizer will incrementally split off punctuation, and keep looking up the remaining substring\n",
    "\n",
    "# gimme! when split it returns 3 values gim, me, ! and lemmas are give, -PRON-, !\n",
    "assert 'gimme' not in [w.text for w in nlp(u'gimme!')] # gimme should not be there in token texts\n",
    "assert [w.lemma_ for w in nlp(u'gimme!')] == [u'give', u'-PRON-', u'!'] # lemmas should be give, -PRON-, !\n",
    "\n",
    "# Tokenizing works even with the string part of the periods and punctuations\n",
    "assert 'gimme' not in [w.text for w in nlp(u'(\"...gimme...?\")')]\n",
    "# Asserting lemmas for '...gimme...?'\n",
    "assert [w.lemma_ for w in nlp(u'...gimme...?')] == [u'...', u'give', u'-PRON-', u'...', u'?'] \n",
    "\n",
    "# Adding another case for matching the whole \"...gimme...?\" to a single token\n",
    "special_case = [{ORTH: u'...gimme...?', LEMMA: u'give', TAG: u'VB'}]\n",
    "nlp.tokenizer.add_special_case(u'...gimme...?', special_case)\n",
    "\n",
    "# the length of tokens should be one\n",
    "assert len(nlp(u'...gimme...?')) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['testing', 'the', '|prefix_and_suffix|', 'here']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "# Observation: |prefix_and_suffix| is taken as single word\n",
    "doc = nlp(u'testing the |prefix_and_suffix| here')\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import regex as re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "# Creating regular Expressions\n",
    "prefix_re = re.compile(r'''^[\\[\\(\"']''')\n",
    "# Observe that | (pipe) is been added as suffix\n",
    "suffix_re = re.compile(r'''[\\]\\)\"|']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "# TODO: Test it with other scenarios of infix and \"token_match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing spaCy's Tokenizer class\n",
    "\n",
    "# defining the custom function\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=simple_url_re.match)\n",
    "\n",
    "# Updating the tokenizer\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['testing', 'the', 'new', '|prefix_and_suffix', '|', 'here']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'testing the new |prefix_and_suffix| here')\n",
    "# Observation: suffix | is separated as another token\n",
    "# TODO: Check why the prefix is not working?\n",
    "print([token.text for token in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
