# ReLU - Rectified Linear Unit

It is an Activation Function

```
f(x) = x+ = max(0, x)
```

where `x` is the input to a neuron.

this is more used in the deep neural networks.